{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdNeWENiiIuR",
        "outputId": "5dcc504b-d514-4a30-a96f-0b107f86df3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of examples in training: 25000\n",
            "Amount of examples in testing: 25000\n",
            "{'text': ['Peaches', 'is', 'truly', 'a', 'marvelous', 'film', '.', 'I', 'write', 'this', 'to', 'refute', 'a', 'review', 'from', 'someone', 'called', \"'\", 'Auscrit', \"'\", ',', 'that', 'has', 'appeared', 'on', 'this', 'site', '.', 'First', 'of', 'all', 'the', 'idea', 'that', 'either', 'Monahans', 'first', 'film', \"'\", 'The', 'Interview', \"'\", 'is', 'somehow', 'TV', 'is', 'an', 'extraordinary', 'statement', '.', 'Here', 'is', 'a', 'film', 'that', 'has', 'been', 'significantly', 'praised', 'around', 'the', 'world', 'as', 'is', 'simply', 'one', 'of', 'the', 'best', 'Australian', 'Films', 'ever', 'made', '.', 'It', 'fully', 'deserved', 'to', 'win', 'best', 'picture', '.', 'Peaches', 'is', 'a', 'brave', ',', 'bold', 'and', 'courageous', 'departure', '.', 'For', 'me', 'it', 'works', 'on', 'every', 'level', 'and', 'I', 'have', 'now', 'seen', 'it', 'twice', '.', 'Monahan', 'is', 'a', 'filmmaker', 'who', 'is', 'demonstrating', 'great', 'skill', 'and', 'incredible', 'sensitivity', '.', 'For', \"'\", 'Auscrit', \"'\", 'to', 'make', 'the', 'comment', 'that', 'it', 'is', 'another', 'TV', 'movie', 'etc', 'and', 'that', 'Hugo', 'Weaving', 'is', 'no', 'good', 'simply', 'does', 'not', \"'\", 'get', \"'\", 'the', 'film', '.', 'Or', 'more', 'particularly', 'does', 'not', 'want', 'to', 'get', 'it', '.', 'Frankly', 'it', 'is', 'the', 'sort', 'of', 'comment', 'that', 'one', 'expects', 'from', 'either', 'another', 'filmmaker', 'who', 'is', 'jealous', 'or', 'bitter', 'or', 'both', '.', 'Or', 'someone', 'from', 'inside', 'the', 'industry', 'either', 'distribution', ',', 'exhibition', 'or', 'bureaucracy', '.', 'Your', 'average', 'punter', ',', 'I', 'have', 'found', 'just', 'does', 'not', 'write', 'comments', 'like', 'that', '.', 'I', 'have', 'noticed', 'other', 'comments', 'on', 'the', 'site', 'and', 'reference', 'to', 'the', 'film', 'Sommersault', '.', 'One', 'has', 'to', 'wonder', 'what', 'people', 'think', 'they', 'are', 'looking', 'at', '.', 'Unfortunately', 'in', 'Australia', 'at', 'the', 'time', 'SS', 'was', 'released', 'the', 'push', 'was', ',', 'if', 'you', 'did', 'not', 'like', 'it', 'then', 'there', 'was', 'something', 'wrong', 'with', 'you', 'not', 'the', 'film', '.', 'This', 'manipulation', 'of', 'the', 'media', 'is', 'pretty', 'common', 'down', 'under', '.', 'The', 'reality', 'is', 'the', 'only', 'similarity', 'between', 'the', 'two', 'films', 'are', 'that', 'they', 'are', 'rights', 'of', 'passage', 'films', '.', 'Unfortunately', 'for', 'me', 'SS', 'is', 'a', 'film', 'about', 'nothing', ',', 'that', 'could', 'have', 'been', 'told', 'in', '15', 'minutes', '.', 'I', 'see', 'it', 'as', 'a', 'one', 'dimensional', 'film', 'about', 'anxiety', '.', 'Peaches', 'in', 'comparison', 'is', 'a', 'master', 'piece', '.', 'Personally', 'I', 'can', 'not', 'wait', 'to', 'what', 'Monahan', 'does', 'next', 'as', 'he', 'is', 'clearly', 'way', 'ahead', 'of', 'any', 'of', 'his', 'contemporaries', 'when', 'it', 'comes', 'to', 'cinema', '.', 'In', 'conclusion', 'if', 'the', 'film', 'does', 'not', 'win', 'all', 'at', 'this', 'years', 'AFI', \"'s\", 'and', 'IF', 'awards', ',', 'then', 'it', 'is', 'a', 'rigged', 'game', '.', 'As', 'for', 'Auscrit', ',', 'please', 'find', 'something', 'more', 'constructive', 'do', 'with', 'your', 'time'], 'label': 'pos'}\n",
            "Amount of examples in training: 17500\n",
            "Amount of examples in validation: 7500\n",
            "Amount of examples in testing: 25000\n",
            "Number of tokens that are unique within the txt vocabulary: 30002\n",
            "Number of tokens that are unique within the lbl vocabulary: 2\n",
            "[('the', 204143), (',', 193928), ('.', 166776), ('and', 109858), ('a', 109747), ('of', 101306), ('to', 94390), ('is', 77113), ('in', 61660), ('I', 54727), ('it', 53805), ('that', 49620), ('\"', 45192), (\"'s\", 43816), ('this', 42614), ('-', 36561), ('/><br', 35794), ('was', 35090), ('as', 30781), ('with', 29906), ('movie', 29703), ('for', 29471), ('film', 27565), ('The', 26408), ('but', 24649), ('(', 23513), (\"n't\", 23305), (')', 23257), ('on', 23161), ('you', 21514)]\n",
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'in', 'I', 'it', 'that', '\"', \"'s\", 'this', '-', '/><br', 'was']\n",
            "defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import anvil.server\n",
        "anvil.server.connect(\"WWR7EBB6FQN3HDIEDA6TGUXJ-5AVW2RHVQQ7H7HHW\")\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as Opt\n",
        "import time\n",
        "import spacy\n",
        "\n",
        " \n",
        "# ///////////////////////////////////////////////////////////////////////////////////\n",
        "# Final Sentiment Analyser\n",
        "# ///////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "# First section\n",
        "\n",
        "# This sentiment analyser will work to obtain digital online texts \n",
        "# and analyse the texts to understand the emotion or \"sentiment\" that they contain\n",
        "\n",
        "# Text Analyser\n",
        "\n",
        "# Sets the access key\n",
        "AccessKey = 5678\n",
        "\n",
        "# Will assign the value of the variable AccessKey to the feature \"manual_seed\"\n",
        "torch.manual_seed(AccessKey)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Begin Tokenisation\n",
        "txt = data.Field(tokenize = \"spacy\", tokenizer_language = \"en_core_web_sm\", include_lengths = True)\n",
        "# The txt variable will be used as a means of defining how the text should be processed\n",
        "# The lbl variable will be used as a means of processing the given sentiment\n",
        "lbl = data.LabelField(dtype = torch.float)\n",
        "\n",
        "# This will download the IMDB dataset and split it up into torchtext.datasets objects\n",
        "Data_Trained, Data_Test = datasets.IMDB.splits(txt, lbl)\n",
        "\n",
        "# By analysing the length, we can find out how many examples are contained in each split\n",
        "print(f'Amount of examples in training: {len(Data_Trained)}')\n",
        "print(f'Amount of examples in testing: {len(Data_Test)}')\n",
        "\n",
        "# Display data from the IMDb database\n",
        "\n",
        "print(vars(Data_Trained.examples[0]))\n",
        "\n",
        "# The variables Data_Trained and Data_Valid are declared and used to contain their respected values\n",
        "Data_Trained, Data_Valid = Data_Trained.split(random_state = random.seed(AccessKey))\n",
        "\n",
        "print(f'Amount of examples in training: {len(Data_Trained)}')\n",
        "print(f'Amount of examples in validation: {len(Data_Valid)}')\n",
        "print(f'Amount of examples in testing: {len(Data_Test)}')\n",
        "\n",
        "# Sets a maximum vocabulary size for the sentiment analyzer\n",
        "Maximum_Size_Of_Vocabulary = 30_000\n",
        "\n",
        "# Sets the vocabulary build, containing the variables for the trained data and maximum size to contain the value\n",
        "# of the variable Maximum_Size_Of_Vocabulary\n",
        "txt.build_vocab(Data_Trained, max_size = Maximum_Size_Of_Vocabulary, vectors = \"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
        "lbl.build_vocab(Data_Trained)\n",
        "\n",
        "# Display the number of tokens that are unique in the txt and lbl variables\n",
        "print(f\"Number of tokens that are unique within the txt vocabulary: {len(txt.vocab)}\")\n",
        "print(f\"Number of tokens that are unique within the lbl vocabulary: {len(lbl.vocab)}\")\n",
        "\n",
        "# Display the words that occur most frequently in the vocabulary including the frequencies that they have\n",
        "print(txt.vocab.freqs.most_common(30))\n",
        "\n",
        "# This will directly display the vocabulary\n",
        "print(txt.vocab.itos[:20])\n",
        "\n",
        "# This will analyse the labels and make sure that positive is represented by 1 and negative is represented by 0\n",
        "print(lbl.vocab.stoi)\n",
        "\n",
        "# Place tensors on the GPU that the iterator has returned\n",
        "# Sets a batch size\n",
        "Size_Of_Batch = 64\n",
        "\n",
        "# Sets an if statement which will return CUDA if it is available, otherwise, it will use the CPU instead\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# The variable Device is then passed on to the iterator\n",
        "Iterator_Trained, Iterator_Valid, Iterator_Test = data.BucketIterator.splits(\n",
        "    (Data_Trained, Data_Valid, Data_Test),\n",
        "    batch_size = Size_Of_Batch, sort_within_batch = True,\n",
        "    device = device)\n",
        "\n",
        "# For this part, a model is built, which will be trained and evaluated\n",
        "# The class has been defined as a recurrent neural network as they are often utilised as a means of analysing sequences\n",
        "class RecurrentNeuralNetwork(nn.Module):\n",
        "  # A function is defined which includes a number of variables such as the Input, Embedding, Hidden, Output etc\n",
        "  # The _init_ will consist of the various layers for the model\n",
        "  def _init_(self, sizeofvocab, Dim_Embedding, Dim_Hidden, Dim_Output, Layers_Num, \n",
        "             Bidirectional, Dropout, Pad_Idx):\n",
        "\n",
        "    super()._init_()\n",
        "\n",
        "  # Will embedd a range of values such as the size of the vocabulary, dim embedding and the pad idx\n",
        "    self.embedding = nn.Embedding(sizeofvocab, Dim_Embedding, Padding_Idx = Pad_Idx)\n",
        "\n",
        "    self.rnn = nn.LSTM(Dim_Embedding, \n",
        "                       Dim_Hidden, \n",
        "                       Layers_N = Layers_Num, \n",
        "                       Birectional = Bidirectional, \n",
        "                       Dropout = Dropout)\n",
        "\n",
        "    self.fc = nn.linear(Dim_Hidden * 2, Dim_Output)\n",
        "\n",
        "    self.Dropout = nn.dropout(Dropout)\n",
        "\n",
        "  def Forward(self, txt, Lengths_Text):\n",
        "      # txt = [Length of text sent, Size of batch sent]\n",
        "\n",
        "      Embedded = self.Dropout(self.Embedding(txt))\n",
        "      # Embedded = [Length of text sent, Size of batch sent, dim embedded]\n",
        "      \n",
        "      # Sequence Pack\n",
        "      Embedded_Pack = nn.utils.rnn.pack_padded_sequence(Embedded, Lengths_Text.to('cpu'))\n",
        "      Output_Pack, (Hidden, Cell) = self.rnn(Embedded_Pack)\n",
        " \n",
        "      # Sequence Unpack\n",
        "      Output, Lengths_Output = nn.utils.rnn.pad_packed_sequence(Output_Pack)\n",
        "\n",
        "      # Output = [length sent, size of batch, dim hidden * directions num]\n",
        "      \n",
        "      # Hidden = [layers num * directions num, size of batch, dim hidden]\n",
        "      # Cell = [layers num * directions num, size of batch, dim hidden]\n",
        "\n",
        "      # Concatenation of final forward (Hidden[-2,:,:]) and backward (Hidden[-1,:,:]) layers hidden\n",
        "      # plus applying Dropout\n",
        "\n",
        "      Hidden = self.Dropout(torch.cat((Hidden[-2,:,:], Hidden[-1,:,:]), Dim = 1))\n",
        "      # Output = [Length of text sent, Size of batch sent, dim hidden]\n",
        "      # Hidden = [size of batch, dim hidden * directions num]\n",
        "\n",
        "      return self.fc(Hidden)\n",
        "\n",
        "      # The input field will be equal to the size of the vocabulary\n",
        "      # The embedding field will be the size of word vectors that are dense\n",
        "      # The hidden field will be the size of the hiiden sections\n",
        "      # The output field is generally the amount of classes\n",
        "      DIM_INPUT = len(TXT.vocab)\n",
        "      DIM_EMBEDDING = 100\n",
        "      DIM_HIDDEN = 256\n",
        "      DIM_OUTPUT = 1\n",
        "      LAYERS_NUM = 2\n",
        "      BIDIRECTIONAL = True\n",
        "      DROPOUT = 0.5\n",
        "      PAD_IDX = TXT.vocab.stoi[TXT.Token_Pad]\n",
        "      # Sets the model to contain the class RecurrentNeuralNetwork with all of its variables \n",
        "      Model = RecurrentNeuralNetwork(DIM_INPUT, \n",
        "                                     DIM_EMBEDDING, \n",
        "                                     DIM_HIDDEN, \n",
        "                                     DIM_OUTPUT, \n",
        "                                     LAYERS_NUM, \n",
        "                                     BIDIRECTIONAL, \n",
        "                                     DROPOUT, \n",
        "                                     PAD_IDX)\n",
        "\n",
        "      # Function to contain the number of parameters within the analyser\n",
        "  def ParameterCount(Model):\n",
        "    # Will return the variable Total\n",
        "      return sum(p.Numel() for p in Model.Parameters() if p.NeedsGrad)\n",
        "      # Will print out a message indicating the number of training parameters which the model contains\n",
        "      print(f'The model contains {ParameterCount(Model):,} parameters training')\n",
        "  # Pretrained word embeddings will then be copied into the embedding layer \n",
        "  # Once the embeddings from the vocab have been gathered, they will then be checked to ensure that they are of the correct size\n",
        "      Embeddings_Pretrained = TXT.vocab.vectors\n",
        "      print(Embeddings_Pretrained.shape)\n",
        "\n",
        "  # The initial weights of the embedding layer are then replaced with embeddings that are pretrained\n",
        "      Model.Embedding.weight.data.copy_(Embeddings_Pretrained)\n",
        "  \n",
        "      Unk_Idx = TXT.vocab.stoi[TXT.unktoken]\n",
        "  # The row of the embedding weights matrix will be set to zero. The row will be discovered through locating the tokens index \n",
        "      Model.Embedding.weight.data[Unk_Idx] = torch.zeros(Dim_Embedding)\n",
        "      Model.Embedding.weight.data[Pad_Idx] = torch.zeros(Dim_Embedding)\n",
        "\n",
        "      print(Model.Embedding.weight.data)\n",
        "     # The model will now be prepared for training\n",
        "     # The optimizer will be used in order to update the modules parameters\n",
        "     # The feature 'Adam' will adapt the rate of learning for each of the parameters, providing paramters which are updated regularly\n",
        "     # with a learning rate that is lower whilst providing parameters that are updated less often with a learning rate that is higher\n",
        "      Optimizer = Opt.Adam(Model.Parameters())\n",
        "     # The criterion will be used as the loss function\n",
        "     # In this case, the Binary Cross Entropy with Logits will act as the loss function\n",
        "     # The Binary_Cross_Entropy_With_Logits() will carry out both the Binary Cross Entropy steps as well as the sigmoid\n",
        "      Criterion = nn.Binary_Cross_Entropy_With_Logits()\n",
        "     # Through the use of .to the model and criterion can be placed onto the GPU\n",
        "      Model = Model.to(device)\n",
        "      Criterion = Criterion.to(device)\n",
        "\n",
        "      # This function will calculate the accuracy\n",
        "  def BinaryAccuracy(Prediction, x):\n",
        "      # This will round the relevant predictions to the nearest integer\n",
        "      # Will return the accuracy of each of the batches\n",
        "      Predictions_Rounded = torch.Round(torch.sigmoid(Prediction))\n",
        "      # The function will pass the values through a sigmoid layer\n",
        "      # It will then produce a calculation of the number of predictions which equal the labels \n",
        "      # and give an average through the batch\n",
        "      Correct = (Predictions_Rounded == x).float()\n",
        "      Acc = Correct.Total() / len(Correct)\n",
        "      # Will return the variable Acc\n",
        "      return Acc\n",
        "\n",
        "       # This function will iterate over each example, going over a batch at a time\n",
        "  def Model_Train(Model, Criterion, Iterator, Optimizer):  \n",
        "\n",
        "      # Will define two variables, Loss_Of_Epoch and Acc_Epoch\n",
        "      Loss_Of_Epoch = 0\n",
        "      Acc_Epoch = 0\n",
        "\n",
        "      # The model.train will be utilised as a means of putting the model into \"training mode\"\n",
        "      # This will activate both, dropout and batch normalisation\n",
        "      Model.Model_Train()\n",
        "\n",
        "      # Will set a for loop that will loop over the batches\n",
        "      for batch in Iterator:\n",
        "\n",
        "        # With each of the batches, the gradient will be zero \n",
        "        # Every parameter will contain a grad attribute that will be used to store the gradient that has been \n",
        "        # calculated through the use of the criterion\n",
        "        Optimizer.zero_grad()\n",
        "        txt, Lengths_Text = batch.text        \n",
        "        # The batch will then be given a set of sentences via Batch.txt that will be placed within the model\n",
        "        # The squeeze will be used to get rid of the size 1 dimension\n",
        "        Predict = Model(txt, Lengths_Text).squeeze(1)\n",
        "        # The Batch.lbl will then calculate the loss and accuracy\n",
        "        Loss = Criterion(Predict, batch.lbl)\n",
        "        Acc = BinaryAccuracy(Predict, batch.lbl)\n",
        "        # Loss.Backward() will then produce a calculation of the gradient of each parameter\n",
        "        # the parameters will then be updated through using the gradients as well as the optimizer algorithm with Optimizer.step()\n",
        "        Loss.backward()\n",
        "        Optimizer.step()\n",
        "        # The .item() method is utilised as a means of extracting a scalar through a tensor that holds only one value\n",
        "        Loss_Of_Epoch += Loss.item()\n",
        "        Acc_Epoch += Acc.item()\n",
        "\n",
        "      # This will return both, the loss and the accuracy that has been averaged over the epoch\n",
        "      # the len will be the amount of batches that the iterator has\n",
        "      return Loss_Of_Epoch / len(Iterator), Acc_Epoch / len(Iterator)\n",
        "\n",
        "      # This will set an evaluation function\n",
        "  def Eval(Model, Criterion, Iterator):\n",
        "    \n",
        "      # Declares two variables, Loss_Of_Epoch and Acc_Epoch and will set them to the value of 0\n",
        "      Loss_Of_Epoch = 0;\n",
        "      Acc_Epoch = 0;\n",
        "\n",
        "      # The model.eval() will place the model into \"evalutation mode\"\n",
        "      # This will deactivate dropout as well as batch normalisation\n",
        "      Model.eval()\n",
        "\n",
        "      # The with no_grad() will stop gradients from being calculated on PyTorch operations within this field\n",
        "      with torch.no_grad():\n",
        "        \n",
        "        # Sets a for loop to iterate throughout the batch\n",
        "        for batch in Iterator:\n",
        "\n",
        "          # Wil assign various variables to equal certain values          \n",
        "          txt, Lengths_Text = batch.Text\n",
        "\n",
        "          # The variable Predict will be given the value of model, \n",
        "          # Loss will be given the value of Criterion and Acc the value of BinaryAccuracy\n",
        "          Predict = Model(txt, Lengths_Text).squeeze(1)\n",
        "\n",
        "          Loss = Criterion(Predict, batch.lbl)\n",
        "\n",
        "          Acc = BinaryAccuracy(Predict, batch.lbl)\n",
        "\n",
        "          Loss_Of_Epoch += Loss.item()\n",
        "          Acc_Epoch += Acc.item()\n",
        "\n",
        "      # Will return the value of the variable Loss_Of_Epoch and Acc_Epoch when being divided by the length of\n",
        "      # the iterator\n",
        "      return Loss_Of_Epoch / len(Iterator), Acc_Epoch / len(Iterator)\n",
        "\n",
        "\n",
        "      # Sets a function to find out the length of time which an epoch will take when comparing various training times\n",
        "      # between models\n",
        "  def Time_Epoch(Time_Begins, Time_Ends):\n",
        "      # These will be used as a means of training the models from a number of epochs \n",
        "      # An epoch will be treated as a whole pass through every example in both, the training and validation sets\n",
        "      Time_Elapsed = Time_Ends - Time_Begins\n",
        "      Minutes_Elapsed = int(Time_Elapsed / 60)\n",
        "      Seconds_Elapsed = int(Time_Elapsed - (Minutes_Elapsed * 60))\n",
        "      # Will return the variables \"Minutes_Elapsed\" and \"Seconds_Elapsed\"\n",
        "      return Minutes_Elapsed, Seconds_Elapsed\n",
        "\n",
        "      # The model is then trained\n",
        "      Epochs_N = 5\n",
        "\n",
        "      GreatestLoss_Valid = float('inf')\n",
        "\n",
        "      for Epoch in Range(Epochs_N):\n",
        "\n",
        "        Time_Begins = time.time()\n",
        "\n",
        "        Loss_Train, Acc_Train = Model_Train(Model, iterator_train, Optimizer, Criterion)\n",
        "        Loss_Valid, Acc_Valid = Eval(Model, iterator_valid, Criterion)\n",
        "\n",
        "        Time_Ends = time.time()\n",
        "        Minutes_Epochs, Seconds_Epochs = Time_Epoch(Time_Begins, Time_Ends)\n",
        "\n",
        "        # If statement which will assign the variable \"GreatestLoss_Valid\" to that of the variable \"Loss_Valid\" in the event \n",
        "        # that the variable \"Loss_Valid\" is less than the variable \"GreatestLoss_Valid\"\n",
        "        if Loss_Valid < GreatestLoss_Valid:\n",
        "          GreatestLoss_Valid = Loss_Valid\n",
        "          torch.save(model.state_dict(), 'TUT2-model.pt')\n",
        "\n",
        "      print(f'Epoch: {Epoch+1:00} | Time_Epoch: {Minutes_Epochs}m {Seconds_Epochs}s')\n",
        "      print(f'\\tModel_Train Loss: {Loss_Train:.3f} | Acc_Train: {Acc_Train*100:.2f}%')\n",
        "      print(f'\\t Value. Loss: {Loss_Valid:.3f} | Value. Acc: {Acc_Valid*100:.2f}%')\n",
        "\n",
        "      Model.State_Load_Dict(torch.Load(\"TUT2-Model.pt\"))\n",
        "\n",
        "      Loss_Test, Acc_Test = evaluate(Model, Criterion, iterator_test)\n",
        "\n",
        "      print(f'Loss Test: {Loss_Test:.3f} | Acc_Test: {Acc_Test*100:.2f}%')\n",
        "\n",
        "\n",
        "      nlp = spacy.load('en_core_web_sm')\n",
        "     \n",
        "\n",
        "  @anvil.server.callable \n",
        "       # This will define a function that will calculate a prediction for the sentiment\n",
        "  def Sentiment_Predictor(Model, Sentence):\n",
        "      Model.eval()\n",
        "      Tokenized = [tok.txt for tok in nlp.Tokenizer(Sentence)]\n",
        "      Indexed = [txt.vocab.stoi[T] for T in Tokenized]\n",
        "      Length = [len(Indexed)]\n",
        "      Tensor = torch.LongTensor(Indexed).to(device)\n",
        "      Tensor = Tensor.unsqueeze(1)\n",
        "      TensorLength = torch.LongTensor(Length)\n",
        "      Predict = torch.sigmoid(Model(Tensor, TensorLength))\n",
        "      # Will return a sentiment prediction for the given text it has been provided with\n",
        "      return Predict.item()\n",
        "\n",
        "      Sentiment_Predictor(Model, \"This film is terrible\")\n",
        "\n",
        "      Sentiment_Predictor(Model, \"This film is good\")\n",
        "\n",
        "      anvil.server.wait_forever()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFMYC7a9vLQ4"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# @anvil.server.callable\n",
        "def Sentiment_Predictor(Model, Sentence):\n",
        "  Model.eval()\n",
        "  Tokenized = [tok.txt for tok in nlp.Tokenizer(Sentence)]\n",
        "  Indexed = [txt.vocab.stoi[T] for T in Tokenized]\n",
        "  Length = [len(Indexed)]\n",
        "  Tensor = torch.LongTensor(Indexed).to(device)\n",
        "  Tensor = Tensor.unsqueeze(1)\n",
        "  TensorLength = torch.LongTensor(Length)\n",
        "  Predict = torch.sigmoid(Model(Tensor, TensorLength))\n",
        "  return Predict.item()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SentimentAnalyserPrototype1.1ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
